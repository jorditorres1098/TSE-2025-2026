\documentclass{article}
\usepackage{amsmath} 
\usepackage{amsfonts}
\usepackage{booktabs}
\usepackage[a4paper, margin=2.5cm]{geometry}
\usepackage{float}   % for [H]
\usepackage{graphicx}   % for \includegraphics
\usepackage{tabularx}
\usepackage[utf8]{inputenc}
\usepackage{geometry}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{blindtext}
\usepackage{hyperref}
\usepackage{natbib} % <-- NEW: to handle references
\usepackage{setspace}
\usepackage{array}
\usepackage{dcolumn}
\usepackage{threeparttable}
\usepackage{tikz}
\usepackage{amsmath}
\usetikzlibrary{decorations.pathreplacing}
\usepackage{pdflscape} % in your preamble
\usepackage{tabularray}
\setcounter{secnumdepth}{2}
\usepackage{amsmath, amsthm}  % for math and theorem environments



\begin{document}

\title{Referee Report}
\author{Jordi Torres}
\date{\today}


\maketitle


\section*{1}
In this paper, Larroucau and Ríos develop a structural model to understand how the decisions of the students to drop out or change courses are made during higher education. They focus on the chilean context, where students apply to college to a centralized system and are allowed to change degrees. Main variation outcome: 21\% in the sample switch, 15\% drop out. They highlight two important channels that can explain this phenomenon: 1. initial mismatching: students who are not assigned to their prefered option may go into other non-prefered options and later switch; 2. learning: students who learn about their skills or preferences in college and switch because of this (i.e they were correctly matched but then change their taste). These two channels have different implications for policy: the first one can be solved by penalizing later switches, so that student's are forced to internalize the negatives externalities imposed on the others later on. The second, on the contrary, would mean that more flexibility and ease of switching needs to be implemented. 

Their contribution is a model with which to separate these two channels + add enough heterogeneity to make the model more robust

\section*{2}
The essential variation that the paper exploits is the almost random variation that occurs around the cutoff of entry grades. In X there is a centralized mechanism to distribute students into college degrees based on their weighted grades from high school. Students above the treshhold are admitted and can accept or reject; students below the threshold are either waitlisted or rejected. They use the variation around the cutoff (comparing students with very similar scores) to argue that there is a causal effect of being shortlisted to the top preference on the probability of switching, dropping out etc. 

Basically, they estimate the following RDD:

\[
y_{bp}=f_{p}(d_{bp}) + \delta_{p} \cdot Z_{bp}+ \epsilon_{bp}
\]

Where $y$ is the outcome of interest (dropout, switch after 1 year \dots etc) in bin of distance to cutoff $b$ applying to program $p$. Z is simply the indicator function of above and below the cutoff (ITT) and   $f_{p}$ is a flexible polynomial of the distance.

This variation is essential for the paper because it allows them to later discipline the separation between channel 1 and channel 2 in the main model. While it is not clear what type of missmatch this is capturing —since the reduced-form effect may pull together effort responses, motivation, tastes, or expectations— it is clear that this missmatch is realized before any learning (mechanism 2) based on grades and performance-driven is gained. In this sense, the RD captures the initial mismatch effect on the continuation value of the assigned option (not enrollment!-> in beliefs, options at hand etc.).


However, the paper does not do an excellent job justifying the validity and interpretation of this central result. First, they do not explicitly state which RD method they use (it appears to be in the spirit of Cattaneo et al.), and they devote limited attention to robustness with respect to polynomial order, bandwidth choice, or placebo tests. Second, the paper is not fully clear about the sharp versus fuzzy nature of the design. Given the presence of waitlists and the fact that crossing the cutoff does not mechanically imply enrollment in the top choice, the design is not sharp. The authors seem to deliberately focus on the ITT effect of eligibility or priority rather than a LATE of actual enrollment, which makes sense if the problem of who are the compliers wants to be avoided, but some discussion on it would be nice. 


\section*{3}
The essential feature of their model is how learning about abilities and preferences is modelled. Although there are other important elements in the model, such as strategic reporting of preferences in ROL, reapplication process etc, I will mostly focus on the learning part, as it is the crucial contribution (the rest is noise!).

The central assumption is that students enter college with imperfect information about their match-specific abilities in a given major $j$ and their preferences over major to study: ability goes all by grades and the rest enters the preference dimension as a utility shifter. One of the fundamental ideas of the paper is how to separate these two dimensions and why?

\textbf{Learning about preferences}
Ability of student i for major m is defined:
\[
\{\alpha_{i,m,t}\}_{m \in M} \sim N(\mu_{i,m}^{a}, \sigma_{i,m}^{a})
\]
Student idyiosyncratic preferences evolve according to 

\[
\alpha_{i, m, t+1}= \alpha_{i,m,t} + \mathbf{1}_{m=m(j)}\cdot \epsilon_{i,m,t}^{a}
\]
where the error depends on the variance. \(\rightarrow\) this explains everything that is not in embedded in ability as signaled by grades.

\textbf{Learning about abilities}

\[
A_{ij}= \sum_{s\in S} \omega_{j,s}A_{i,s}^{0} +  \sum_{s\in S} \omega_{j,s}A_{i,s}^{p} + A_{i,m(j)}^{p} + \sum_{s\in S} \omega_{j,s}A_{i,s}^{u} + A_{i,m(j)}^{u}
\]

Where $A_{i,s}^{0}$ is an observable component observed by the researcher and the student, $A_{i,m(j)}^{p}$ is a private measure of ability only observed by the student and $A_{i,m(j)}^{u}$ is the part of ability unknown by the student.

They allow students to have potentially biased initial beliefs on the unknown component of ability. That is, the mean and variance of $A_{i,m(j)}^{u}$ depend on $\tilde{\mu}_{i,k,t}$ and $\tilde{\sigma}_{i,k,t}^{2}$ , the expected grades.-->maybe add update rule.

\textbf{How ability enters the model}

Ability on grades enters different elements of the model. It enters the grade function: \(G_{i,j,t}=G(X_{i,j}^{G}, A_{i,j}^{p}, A_{i,j}^{u}, \epsilon_{i,j,t}^{G})\)\footnote{Here they use $\tilde{G_{ijt}}$ as the outcome of interest,as they observe the beliefs of the students about the grades they will achieve in the following year ; so they check \(\mathbf{E}(G_{ijt})\), where the expectation is taken over the uncertain ability and the utility shock.}. 

The labor market function \(V_{i,j,t}= \beta_{m(j)}^{v}+ \beta_{a}^{v}a_{i, m(j), t'} + \beta_{a}^{V}A_{i,j}^{0}+ \beta_{pu}^{V}(A_{i,j}^{p}+A_{i,j}^{u})+ \beta_{w}^{v}\log(\mathbf{E}\left[\sum_{t=t'}^{T} \beta^{t-t'} P_{m(j),t}^{w} W_{i,j,t}\right])\), the expected returns in labor market.

While the preferences enter only as random effects in the per period flow utility: \(U_{i,j,t}= U(X_{i,j}^{U}, \alpha_{i, m(jt)}, \epsilon_{i,j,t}^{U})\)



\textbf{Identification}

\textbf{Private ability:}
 Following Arcidiacono (2005), students are assumed to know their ability when ranking their preferences. They also assume that students rank their preferences based on their comparative advantage, which is unobserved by the researcher. The idea is that students rank options $j \in J$, where J is ROL, based on relative utility:

Choose $j>j' \iff U_{ij}>U_{ij'}$, then define utility:

\[
U_{ij}= \gamma_j + A_{ij}^{p}+ Z_{ij}\phi_{j} + \eta_{ij}
\]
Where  $A_{ij}^{p}$ is the private utility and can be modelled by a set of random coefficients. With this they identify \(\phi^{2} \sigma^{2}_{A_{ij}^p}/\sigma^{\nu_{ij}}\) up to scale. The idea is that this first stage can be used then as a control function approach to estimate the grade equation:

\[
G_{i,j,t}=G(X_{i,j}^{G}, A_{i,j}^{p}, A_{i,j}^{u}, \epsilon_{i,j,t}^{G})
\]

This equation is biased if we don't control for the fact that students sort based on comparative advantage. The idea is to use the probability of ranking options as a control function approach and do (they also assume linearity): 

\[
G_{i,j,t}= \beta_{0} + \beta X_{i,j}^{G} + s(Pj) + A_{i,j}^{u} + \epsilon_{i,j,t}^{G}
\]

where $Pj=P(j=j |X, Z)$ and under the control function restriction $\mathbf{E}(A_{i,j}^{u} + \epsilon_{i,j,t}^{G}| j, X, Z)=s(Pj)$ . Such that now $\mathbf{E}(\epsilon_{i,j,t}^{G}| s)=0$ and if we take the error of this, we get the $A_{i,j}^{u} + \epsilon_{i,j,t}^{G}$ that is used in the next step. 

\textbf{Unknown ability and grade shock:}

Now the idea is how to disentangle the two. With just
\[
a_{i,j,t}= A_{i,j}^{u}+ \epsilon_{i,j,t}^{G}
\]
we cannot separate unknown ability and grade noise if we only observe the grade signal (grades net of private ability and observed characteristics). They identify the two by using only data from non-switchers, imposing an i.i.d. shock assumption and normality assumptions.

Conditional on staying in the same major in period 1 and 2:
\[
V(a_{ij1}\mid j_{i1}= j_{i2}=j)
=
V(A_{ij}^{u}\mid j_{i1}= j_{i2}=j)
+
V(\epsilon_{ij1}^{G}\mid j_{i1}= j_{i2}=j),
\]

\[
V(a_{ij2}\mid j_{i1}= j_{i2}=j)
=
V(A_{ij}^{u}\mid j_{i1}= j_{i2}=j)
+
V(\epsilon_{ij2}^{G}\mid j_{i1}= j_{i2}=j).
\]

Under the i.i.d. shock assumption,
\[
V(\epsilon_{ij2}^{G}\mid j_{i1}= j_{i2}=j)
=
V(\epsilon_{ij1}^{G}\mid j_{i1}= j_{i2}=j)
=
\sigma^{2}_{G}.
\]

We still have two unknown components. The key additional restriction comes from the fact that in period 1 staying is a decision taken after observing \(a_{ij1}\), which implies self-selection among those who remain in the same major. As a consequence, the distribution of \(a_{ij1}\) conditional on staying is truncated. Under normality,
\[
V(a_{ij1}\mid j_{i1}= j_{i2}=j)
=
\left( V(A_{ij}^{u})+\sigma^{2}_{G} \right)\cdot \kappa,
\]
where \(\kappa\in(0,1)\) is a known function of the truncation point implied by the stay decision. In contrast, the second-period signal \(a_{ij2}\) is realized after the stay decision and is therefore not truncated with respect to its own shock. Therefore,
\[
V(a_{ij2}\mid j_{i1}= j_{i2}=j)
=
V(A_{ij}^{u})+\sigma^{2}_{G}.
\]

Taken together, the truncated variance of the first-period signal and the untruncated variance of the second-period signal among non-switchers form a system of two equations in two unknowns, which allows separate identification of the variance of unknown ability \(V(A_{ij}^{u})\) and the variance of the grade shock \(\sigma^{2}_{G}\).

\textbf{prior beliefs} This is easier, they identify the prior on unknown ability $\tilde{\mu_{i,j}}$ by taking the difference between the expected grades and the realized grades in year 1 + measurement error assume to be $N(0,\sigma_{G}^{2})$ and identified above. Variances are identified are identified by exploting a quesiton on probability that their grades fall between certain brackets. 

\textbf{Learning about preferences and ability.}

Learning is identified using the correlation between grade signals and students’ choices after the first period, including switching and dropout decisions.

The key idea is that the responsiveness of choices to observed grades depends on the signal-to-noise ratio. A high signal-to-noise ratio (low grade-noise variance relative to the variance of unknown ability) implies that grades are more informative, so students update their beliefs more strongly and choices respond more sharply to realized grades. This translates into stronger correlations between first-year grades and switching behavior.

Finally, the variance of random taste shocks (or preference shocks) is identified by focusing on students who reapply or change their choices after receiving a non-informative grade signal. Conditional on a signal that is uninformative about ability, changes in choices must be driven by preference shocks rather than learning about ability. Variation in these preference changes across majors identifies the standard deviation of the evolution of
random coefficients, denoted \(\zeta_{\alpha}\). \footnote{Omitted in this section is all the heterogeneity that they include in the analysis, which is one of the strengths. For simplicity, I did not talk about it}


\textbf{Counterfactuals on information provision}

One of the two main goals of the paper is to understand how choices of the students would change if information provision changed. More precisely, at the core of their counterfactuals is to quantify the extend towards which missinformation in preferences or/and ability account for the decision to switch major 

They simulate these two scenarios:

\begin{enumerate}
    \item Set \(\zeta_{\alpha}=0\). This they interpret as the effect of counseling in high-school: reduce uncertainty of preferences-->really?
    \item Set \(\tilde{\mu}_{i,j}=0, \tilde{\sigma_{i,j}^{2}=0}\) and move all uncertainty to private ability $\xi^{2}_{i,j} = \hat{\xi_{i,j}}^{2} + \sigma_{i,j}^{2}$ effect of pre-higher education training
\end{enumerate}

Importantly, the cf are bullshit, but at least the heterogeneity is impressive. Imposing heterogeneity at the model level is essential to understand differential effects (if not, in nonlinear models, we would still find heterogeneity if we condition results by characteristics, but we would not know why\dots)

\section*{4}

Without the RD, it would be extremely difficult to disentangle the effect of initial mismatch from subsequent learning. Also,  in the model, counterfactuals necessarily involve changes in admission cutoffs, which mechanically generate mismatch between students and programs. For this reason, identifying the causal effect of being assigned to a higher-ranked option at the margin is essential to discipline the magnitude of mismatch effects in the model. While the authors somewhat downplay this point, the RD is in fact central: it provides quasi-experimental variation that anchors the effect of initial assignment independently of the learning mechanism.

Without the structural model, on the other hand, the RD would only capture a very local and uninformative treatment effect. The RD estimates the impact of barely crossing an admission cutoff on outcomes such as switching or dropout, but it does not explain why students switch, how beliefs evolve, or how these local effects extrapolate to other margins. On its own, the RD cannot separate preference mismatch from learning about ability, nor can it be used to evaluate counterfactual admission rules.

In short, without the RD the structural model would be weakly identified and rely heavily on functional-form assumptions, while without the structural model the RD would remain a local result with limited economic interpretation and interest. The contribution of the paper lies in the interaction between the two: the reduced-form RD provides credible causal variation, and the structural model uses -maybe not sufficiently explicitly- that variation to understand mechanisms and conduct policy-relevant counterfactuals.



\section*{5}
modeling heterogeneity with detail, how they separate initial missmatch with reapplication etc. Initial missmatch and discipline the model thorugh that variation...this helps the second set of CF well. 

\section*{6. Main Concerns}

I see five main issues.

\medskip

\noindent
\textbf{1. Mechanical separation between ability and preferences.}  
The distinction between learning about ability and learning about preferences is largely mechanical. Ability learning operates through updates to $A_{ij}^u$, driven by grade surprises $G_{ijt}-\mathbb{E}[G_{ijt}]$. However, interpreting this object as ability learning is unclear: grade realizations may also reflect endogenous effort, motivation, disengagement, or strategic behavior, none of which are modeled. Thus, the grade signal conflates multiple mechanisms. For example, a student who anticipates low returns or plans to reapply may optimally reduce effort in period $t$, generating a negative grade surprise that the model interprets as low ability rather than an endogenous response\footnote{Relatedly, the absence of effort or persistent heterogeneity implies that learning dynamics absorb variation that would typically be captured by unobserved types.}.

What the paper calls preference learning is defined residually as any component of utility not explained by grades. The variance of $\gamma_{ijt}$ is identified using individuals who receive “uninformative” grade signals but still switch. This argument is problematic, since a realization equal to the prior mean is still informative in a Bayesian sense (it reduces posterior variance). More importantly, the result that ability learning dominates preference learning is mechanically implied by the model: grade signal affects grades, continuation values, graduation, and wages, while preferences enter only through flow utility (after conditioning on program fixed effects). Hence the relative importance of the two channels is largely built in by the model.

\medskip

\noindent
\textbf{2. Misinterpretation of bias and weak information counterfactuals.}  
The paper repeatedly interprets deviations between realized and expected grades as evidence of biased beliefs. This is not correct. Under rational expectations with uncertainty, forecast errors are inevitable. Bias would instead correspond to systematic misperceptions of objective quantities, such as graduation probabilities or wage distributions conditional on completing program $j$.

This distinction matters for the information counterfactuals. In the paper, “counseling” is modeled by shutting down preference uncertainty (setting the variance of $\gamma_{ijt}$ to zero). This counterfactual does not reduce inevitable uncertainty about grades, effort, or fit; it simply removes a residual shock from the model. In practice, counseling would more plausibly affect beliefs about objective outcomes (e.g., labor market returns, risks of dropout) or help students articulate tastes, which are not separately identified here. The counterfactual therefore does not map clearly to a realistic information policy, even though the paper correctly highlights that uncertainty and learning play an important role in switching behavior.\textbf{Add also the other CF on information. Here results of the model are based also on modelling assumptions... and provide no information}->since Arcidiacono(2025) we know information matters...

\medskip

\noindent
\textbf{3. Policy-invariant strategic behavior.}  
The second set of counterfactuals relies on a fixed share of strategic applicants, $\rho$. Even if the identification of $\rho$ under the baseline is taken at face value (assumptions is also shaky), assuming that $\rho$ is invariant to policy changes is strong. Application rules plausibly affect incentives to acquire information and behave strategically. Presenting results under alternative values of $\rho$ does not resolve this issue, since all parameters are estimated conditional on a fixed behavioral regime.

\medskip

\noindent
\textbf{4. Switching outside the initial choice set.}  
A striking fact is that about $67\%$ of students who switch move to programs outside their initial rank-order list, compared to only $17\%$ who move within the list. This suggests evolving choice sets or option discovery, rather than reordering preferences over a fixed set of alternatives. This pattern complicates the interpretation of initial mismatch and, surprisingly, is not addressed by the model or considered at all.

\medskip

\noindent
\textbf{5. Stationarity of beliefs across cohorts.}  
Belief and preference data from cohorts surveyed in 2022--2024 are used to identify parameters for the 2014 cohort under a conditional stationarity assumption. Given large changes in macroeconomic conditions, labor market returns, and information environments over this period, this assumption is strong. Since beliefs play a central role in the model (they enter everywhere), deviations from stationarity could materially affect the estimated learning dynamics and counterfactuals.

\medskip

\noindent
Overall, the model places substantial structure on learning while abstracting from effort, evolving choice sets, and policy-dependent beliefs. As a result, several counterfactual conclusions appear to be driven more by modeling assumptions than by well-identified economic mechanisms.


\section*{7}

Information treatment should either be: 
Contrary to Wiswall and Zafar they actually have consistent data on beliefs, they don't find that beliefs or preferences are already stable before entering higher education. But watch out because I think they make an extrapolation that is a big shot-> belief from other generation to this generation!
What is different wrt to Arcidiacono et al. Understand this well. 

Or particularly model how students react to signals...The thing is that there is unresolved uncertainty that can not be avoided... And this paper tries to do too much without gaining anything in return. 

Do a proposal of modelling well information provision treatments well. Considering who gives information, who receives it etc. We know students don't know shit, so let's be serious about it. Cite main examples there. 

\end{document}

