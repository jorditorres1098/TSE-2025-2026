{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87cb4961",
   "metadata": {},
   "source": [
    "This is just to test the basic functions of the Julia language. I will start a project where I practice basic linear algebra and then doing maximization techniques and doing some basic econometrics. I will practice a little bit with the dataset. The idea is the following: \n",
    "# OLS from Scratch in Julia\n",
    "\n",
    "## Problem Setup\n",
    "\n",
    "We want to simulate data from the following linear model:\n",
    "\n",
    "\\[\n",
    "y_i = 1 + 0.5x_{1i} - 0.3x_{2i} + \\varepsilon_i, \\quad i = 1, \\dots, 100\n",
    "\\]\n",
    "\n",
    "where:\n",
    "\n",
    "- \\(x_{1i}, x_{2i} \\sim N(0,1)\\) (independent standard normal draws)  \n",
    "- \\(\\varepsilon_i \\sim N(0, 0.2^2)\\) (random noise)  \n",
    "\n",
    "## Tasks\n",
    "\n",
    "1. **Simulate data**  \n",
    "   - Generate \\(x_{1i}\\) and \\(x_{2i}\\).  \n",
    "   - Construct the design matrix \\(X\\) with a column of ones (for the intercept) and the two regressors.  \n",
    "   - Generate \\(y\\) using the true model.  \n",
    "\n",
    "2. **Estimate parameters manually**  \n",
    "   - Compute the OLS estimator:  \n",
    "     \\[\n",
    "     \\hat{\\beta} = (X'X)^{-1} X'y\n",
    "     \\]  \n",
    "\n",
    "3. **Compare with truth**  \n",
    "   - Compare your estimates \\(\\hat{\\beta}\\) with the true coefficients \\([1, 0.5, -0.3]\\).  \n",
    "\n",
    "4. **Visualization**  \n",
    "   - Plot predicted values \\(\\hat{y}\\) against the actual values \\(y\\).  \n",
    "   - Add a 45° line for reference.  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c4ab382",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First we simply generate the random variables that we will need for our study. We also generate the true dgp. \n",
    "\n",
    "n,k= 100, 3 ;  #define the sample size and the number of parameters that we have. \n",
    "\n",
    "X=[ones(n) rand(n,k-1)] ; #define the main variables. But this should be a matrix n*k\n",
    "epsilon= rand(n)*0.2 ; #n*1 vector\n",
    "\n",
    "true_beta=[1, 0.5, 0.3] ; #k*1 vector\n",
    "\n",
    "y_i=  X*true_beta+ epsilon ; #true dgp n*1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f06cdd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Vector{Float64}:\n",
       " 1.0915610303674592\n",
       " 0.4943292001115156\n",
       " 0.3277593780762066"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Now we generate the OLS coefficients in a simple and elegant way. \n",
    "\n",
    "\n",
    "\n",
    "beta_hat= inv(X'X)*X'y_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bce75f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mModule DataStructures with build ID fafbfcfd-88f5-7ade-0002-3ac56036c601 is missing from the cache.\n",
      "\u001b[33m\u001b[1m│ \u001b[22m\u001b[39mThis may mean DataStructures [864edb3b-99cc-5e75-8d2d-829cb0a9cfe8] does not support precompilation but is imported by a module that does.\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ Base loading.jl:2541\u001b[39m\n",
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mModule DataStructures with build ID fafbfcfd-88f5-7ade-0002-3ac56036c601 is missing from the cache.\n",
      "\u001b[33m\u001b[1m│ \u001b[22m\u001b[39mThis may mean DataStructures [864edb3b-99cc-5e75-8d2d-829cb0a9cfe8] does not support precompilation but is imported by a module that does.\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ Base loading.jl:2541\u001b[39m\n",
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mModule DataStructures with build ID fafbfcfd-88f5-7ade-0002-3ac56036c601 is missing from the cache.\n",
      "\u001b[33m\u001b[1m│ \u001b[22m\u001b[39mThis may mean DataStructures [864edb3b-99cc-5e75-8d2d-829cb0a9cfe8] does not support precompilation but is imported by a module that does.\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ Base loading.jl:2541\u001b[39m\n",
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mModule DataStructures with build ID fafbfcfd-88f5-7ade-0002-3ac56036c601 is missing from the cache.\n",
      "\u001b[33m\u001b[1m│ \u001b[22m\u001b[39mThis may mean DataStructures [864edb3b-99cc-5e75-8d2d-829cb0a9cfe8] does not support precompilation but is imported by a module that does.\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ Base loading.jl:2541\u001b[39m\n",
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mModule StatsBase with build ID ffffffff-ffff-ffff-0002-3b1c135cb909 is missing from the cache.\n",
      "\u001b[33m\u001b[1m│ \u001b[22m\u001b[39mThis may mean StatsBase [2913bbd2-ae8a-5f71-8c99-4fb6c76f3a91] does not support precompilation but is imported by a module that does.\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ Base loading.jl:2541\u001b[39m\n",
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mModule StatsBase with build ID ffffffff-ffff-ffff-0002-3b1c135cb909 is missing from the cache.\n",
      "\u001b[33m\u001b[1m│ \u001b[22m\u001b[39mThis may mean StatsBase [2913bbd2-ae8a-5f71-8c99-4fb6c76f3a91] does not support precompilation but is imported by a module that does.\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ Base loading.jl:2541\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StatsModels.TableRegressionModel{LinearModel{GLM.LmResp{Vector{Float64}}, GLM.DensePredChol{Float64, CholeskyPivoted{Float64, Matrix{Float64}, Vector{Int64}}}}, Matrix{Float64}}\n",
       "\n",
       "y ~ 1 + X1 + X2\n",
       "\n",
       "Coefficients:\n",
       "────────────────────────────────────────────────────────────────────────\n",
       "                Coef.  Std. Error      t  Pr(>|t|)  Lower 95%  Upper 95%\n",
       "────────────────────────────────────────────────────────────────────────\n",
       "(Intercept)  1.09156    0.0154326  70.73    <1e-84   1.06093    1.12219\n",
       "X1           0.494329   0.0208574  23.70    <1e-41   0.452933   0.535725\n",
       "X2           0.327759   0.0193155  16.97    <1e-30   0.289423   0.366095\n",
       "────────────────────────────────────────────────────────────────────────"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using DataFrames, GLM, Plots\n",
    "df = DataFrame(X1 = X[:,2], X2 = X[:,3], y = y_i)\n",
    "ols_model = lm(@formula(y ~ X1 + X2), df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37b70bbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Vector{Float64}:\n",
       " 0.015432617116023863\n",
       " 0.020857415406009237\n",
       " 0.019315514672206074"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#now suppose we want to compute the standard errors of this simple model. \n",
    "using LinearAlgebra\n",
    "\n",
    "epsilon_hat= y_i - X*beta_hat\n",
    "sigma_sqrt= (epsilon_hat'*epsilon_hat)/(n-k)\n",
    "as_covar= sigma_sqrt*inv(X'X) \n",
    "se= sqrt.(diag(as_covar))\n",
    "\n",
    "#assuming heteroskedasticity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b78fc6",
   "metadata": {},
   "source": [
    "Here are the notes from the Julia class volume 2. This is using the Gauss Quadrature package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda8d986",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.7182810043725218"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using FastGaussQuadrature\n",
    "\n",
    "n=2 \n",
    "\n",
    "z,w= gausslegendre(n)\n",
    "f=0.5*exp.((0.5)*(1.0.+z)) ##\n",
    "sum(w.*f)\n",
    "\n",
    "##we approximate quite well the function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3d47fcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.718281828459045"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "exp(1)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f867044",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Add a package for plots \n",
    "using FastGaussQuadrature \n",
    "\n",
    "#change of variables!! \n",
    "n=20\n",
    "p,w=gausslegendre(n)\n",
    "\n",
    "\n",
    "D(p)= (10-(p/2))*(p<=20) ;\n",
    "S(p)=sum((D.(0.5*p*(1.0.+z))*p/2).*w)\n",
    "\n",
    "using PyPlot\n",
    "P=range(0.30, 100)\n",
    "Plot(P,S.(P))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "006ab7eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-element Vector{Float64}:\n",
       " -0.5830627422130561"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##Example 3\n",
    "using FastGaussQuadrature \n",
    "\n",
    "n=11\n",
    "z,w=gausshermite(n)\n",
    "μ=0.1\n",
    "σ=0.5\n",
    "\n",
    "[sum(w.*exp.(sqrt(2)*σ*z.+μ)/sqrt(n))-exp(μ+σ^2/2)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77263693",
   "metadata": {},
   "source": [
    "*Montecarlo approximations* \n",
    "\n",
    "More flexible and allows to approximate less smooth functions...(more intense computationally though...)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486a0a43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×3 Matrix{Float64}:\n",
       " 1.71819  1.71828  -9.07214e-5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "N=100_000000\n",
    "x=rand(N)\n",
    "\n",
    "[sum(exp.(x))/N exp(1)-1 sum(exp.(x))/N-(exp(1)-1) ]\n",
    "\n",
    "##We need a lot of points to actually approximate the function we need. This is super inefficient and takes a lot of time. This is done when there is no alternative (i.e compute the posterior distribution in Bayesian econometrics). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.3",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
