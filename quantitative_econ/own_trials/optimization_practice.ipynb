{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "013668c8",
   "metadata": {},
   "source": [
    "**1. Optimization** Here I simply generate the code to practice a little bit optimization techniques. This is fundamental for my understanding of the programming languages so I need to invest a lot of time on this this year. Either with Python or with Julia. Think it is useful to check both"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96d9374",
   "metadata": {},
   "source": [
    "**1.1 Gradient descent method** This is quite intuitive and is based simply in using the gradient and setting it close to 0. In all cases, what we want to do is find a routine or algorithm that, for a given function $F(x)$, we find $x_{k+1}= x_k + \\sigma_k \\delta_k$. This works well for local optima, though it has slow convergence. Here $\\delta_k=F'(x_k)$, the gradient and $\\lambda>0$ is set arbitrarily -it is not yet endogeneized in the algorithm. \n",
    "\n",
    "Let's try to first replicat the first example given in class. We want to optimize the function \\(F(x,y)=(1-x)^2 + (y-x^{2})^{2}\\). We can solve very easily for the analytical solution, which is simply $(x,y)=(1,1)$. Let's try to see if our method delivers this same function. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d0ce4aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "using ForwardDiff\n",
    "\n",
    "F(x)= (1-x[1])^2 + 100*(x[2]-x[1]^2)^2\n",
    "J= x->ForwardDiff.gradient(F,x)\n",
    "λ=0.0001\n",
    "\n",
    "x0=[0.0,0.0]\n",
    "\n",
    "maxiter=10_000\n",
    "crit=1.0\n",
    "tol=1e-8\n",
    "iter=0\n",
    "\n",
    "while crit>tol\n",
    "    iter+=1\n",
    "    x1= x0- λ*J(x0)\n",
    "    crit=maximum(abs.(x1-x0))\n",
    "    x0= copy(x1)\n",
    "    if crit<=tol\n",
    "    println(round.(x0; digits=6))\n",
    "    end\n",
    "\n",
    "end \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71cb293f",
   "metadata": {},
   "source": [
    "*Now let's try another function with multiple solutions. We can try and see if we can write an aglorithm that uses this method to find the two solutions. As I said before, one of the main limitations of this method is that it does not work well to detect global maximum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaef9a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "using ForwardDiff\n",
    "\n",
    "G(x)= -(x[1]^2 - 1)^2 - (x[2]^2 - 1)^2 ##Gaussian formula\n",
    "J= x->ForwardDiff.gradient(G,x)\n",
    "\n",
    "x0=[0.0,0.0]\n",
    "λ=0.01\n",
    "crit=1.0\n",
    "tol=1e-8\n",
    "\n",
    "maxiter = 10_000\n",
    "\n",
    "if maximum(abs.(J(x0))) < 1e-12\n",
    "    x0 .+= 1 .* randn(2)   # small random perturbation \n",
    "end\n",
    "\n",
    "while crit > tol    \n",
    "    iter +=1\n",
    "    x1=x0+λ*J(x0)\n",
    "    crit=maximum(abs.(x1-x0))\n",
    "    x0=copy(x1)\n",
    "    \n",
    "\n",
    "end \n",
    "\n",
    "##This is an interesting problem because it has a saddle point in 0,0 and we need to perturbate the initial point. Here the issue is that is a local maxima that we keep finding and depending on my perturbation we find 4 different maximums (1,-1), (1,1)...To find them all I need to randomize the direction of the perturbation.\n",
    "\n",
    "#So bottom line: it is important to understand the class of functions and the methods we have at hand to deal with the particular problem efficiently. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf0dbac",
   "metadata": {},
   "source": [
    "**2. Newton method** this is a better method, where we endogeneize lambda and omega."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59749265",
   "metadata": {},
   "outputs": [],
   "source": [
    "using ForwardDiff\n",
    "F(x)= (1-x[1])^2 + 100*(x[2]-x[1]^2)^2\n",
    "J= x->ForwardDiff.gradient(F,x)\n",
    "H= x->ForwardDiff.hessian(F,x)\n",
    "\n",
    "x0=[0.0,0.0]\n",
    "crit=1\n",
    "tol= 1e-8\n",
    "\n",
    "while crit>tol\n",
    "    x1 = x0 - H(x0) \\ J(x0) ##funcó interessant left hand division!! more efficient!\n",
    "    crit=maximum(abs.(J(x0)))\n",
    "    x0=copy(x1)\n",
    "end\n",
    "\n",
    "#This is faster than before but it needs to have a twice differentiable eq etc... not so easy....Also, it is still a method that uses only local approximations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4cc7f67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " * Status: success\n",
       "\n",
       " * Candidate solution\n",
       "    Final objective value:     1.429810e-30\n",
       "\n",
       " * Found with\n",
       "    Algorithm:     BFGS\n",
       "\n",
       " * Convergence measures\n",
       "    |x - x'|               = 1.31e-10 ≰ 0.0e+00\n",
       "    |x - x'|/|x'|          = 1.31e-10 ≰ 0.0e+00\n",
       "    |f(x) - f(x')|         = 7.65e-21 ≰ 0.0e+00\n",
       "    |f(x) - f(x')|/|f(x')| = 5.35e+09 ≰ 0.0e+00\n",
       "    |g(x)|                 = 4.35e-14 ≤ 1.0e-12\n",
       "\n",
       " * Work counters\n",
       "    Seconds run:   0  (vs limit Inf)\n",
       "    Iterations:    17\n",
       "    f(x) calls:    55\n",
       "    ∇f(x) calls:   55\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##Finalment, podem aplicar directament optimization package a Julia. \n",
    "\n",
    "using Optim\n",
    "F = x->(1-x[1])^2+100*(x[2]-x[1]*x[1])^2 \n",
    "x0 = [0.0,0.0]\n",
    "result = optimize(F, x0, BFGS(),\n",
    "                   Optim.Options(g_tol = 1e-12);\n",
    "                   autodiff=:forward)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.3",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
